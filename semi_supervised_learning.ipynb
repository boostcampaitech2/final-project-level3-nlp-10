{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data import load_dataset, punctuation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device = {device}')\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertWordPieceTokenizer('./pretrained_tokenizer/vocab.txt', lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start puncutation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19511 [00:00<?, ?it/s]/opt/ml/project/final-project-level3-nlp-10/data.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['text'][i] = ''.join(text)\n",
      "100%|██████████| 19511/19511 [00:01<00:00, 10980.36it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('labeled.csv')\n",
    "p_df = pd.read_csv('pseudo_labeled2.csv')\n",
    "test_df = pd.read_csv('test_.csv')\n",
    "\n",
    "df = punctuation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4647, 4459)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label = df[df['label']==1]\n",
    "false_label = df[df['label']==0].sample(frac=0.3)\n",
    "len(true_label), len(false_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = true_label.reset_index().drop(['index'],axis=1)\n",
    "false_label = false_label.reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = true_label.append(false_label)\n",
    "labels = list(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14920, 16341)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_true_label = p_df[p_df['label']==1]\n",
    "p_false_label = p_df[p_df['label']==0].sample(frac=0.1)\n",
    "len(p_true_label), len(p_false_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_label = p_true_label.reset_index().drop(['index'],axis=1)\n",
    "p_false_label = p_false_label.reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = p_true_label.append(p_false_label)\n",
    "p_labels = list(p_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_sentence(tokenizer, df):\n",
    "    tokenized = []\n",
    "    for text in list(df['text']):\n",
    "        tokens = tokenizer.encode(text).ids\n",
    "        if len(tokens) <= 200:\n",
    "            for i in range(200-len(tokens)): tokens.append(0)\n",
    "        elif len(tokens) > 200:\n",
    "            for i in range(len(tokens)-200): tokens.pop()\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----labeled data tokenizing-----\n",
      "----unlabeled data tokenizing-----\n",
      "----test data tokenizing-----\n",
      "====Finish====\n"
     ]
    }
   ],
   "source": [
    "print('----labeled data tokenizing-----')\n",
    "df = tokenized_sentence(tokenizer, df)\n",
    "\n",
    "print('----unlabeled data tokenizing-----')\n",
    "p_df = tokenized_sentence(tokenizer, p_df)\n",
    "\n",
    "print('----test data tokenizing-----')\n",
    "test_df = tokenized_sentence(tokenizer, test_df)\n",
    "\n",
    "\n",
    "dataset = load_dataset(df, labels)\n",
    "p_dataset = load_dataset(p_df, p_labels)\n",
    "test_dataset = load_dataset(test_df, test_labels)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "p_dataloader = DataLoader(p_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "print('====Finish====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 12, 14112, 36, 65, 24149, 18981, 65, 42292, 13, 42307, 4979, 5182, 284346, 24866, 4955, 5573, 6180, 5083, 11737, 26618, 5016, 317188, 126904, 20078, 5012, 5426, 12011, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n"
     ]
    }
   ],
   "source": [
    "print(df[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([     2,     12,  14112,     36,     65,  24149,  18981,     65,  42292,\n",
       "             13,  42307,   4979,   5182, 284346,  24866,   4955,   5573,   6180,\n",
       "           5083,  11737,  26618,   5016, 317188, 126904,  20078,   5012,   5426,\n",
       "          12011,      3,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]),\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Config, set_seed\n",
    "set_seed(42)\n",
    "\n",
    "config = Config(\n",
    "    dropout1=0.3,\n",
    "    dropout2=0.4,\n",
    "    learning_rate=0.001,\n",
    "    label_smoothing=0.5,\n",
    "    epochs=50,\n",
    "    embedding_dim=100,\n",
    "    channel=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lightweight/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "vocab_size = 500000\n",
    "print(f'vocab size = {vocab_size}')\n",
    "model = modeling.Model(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=config.embedding_dim, \n",
    "    channel=config.channel, \n",
    "    num_class=2,\n",
    "    dropout1=config.dropout1,\n",
    "    dropout2=config.dropout2)\n",
    "model.to(device)\n",
    "\n",
    "# set criterion, optimizer, schdeuler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "epochs = 100\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in trange(epochs):\n",
    "    running_loss = 0\n",
    "    for i, labeled in enumerate(dataloader):\n",
    "        input = labeled['input_ids'].to(device)\n",
    "        label = labeled['label'].to(device)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Train Loss : {running_loss/len(dataloader):.5f}')\n",
    "torch.save(model.state_dict(), f'./pseudo/result_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_weight(step):\n",
    "    # T1, T2 : hyperparameters\n",
    "    T1 = 100\n",
    "    T2 = 700\n",
    "    af = 3.0\n",
    "    if step < T1:\n",
    "        return 0.0\n",
    "    elif step > T2:\n",
    "        return af\n",
    "    else:\n",
    "         return ((step-T1) / (T2-T1))*af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.75it/s][03:12<2:54:02, 21.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss : 0.01293 | Test Acc : 0.99686 | Zero : 2652 | One : 2755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.55it/s] [06:45<2:50:05, 21.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss : 0.00337 | Test Acc : 0.99963 | Zero : 2665 | One : 2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.33it/s] [10:19<2:46:57, 21.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Train Loss : 0.00088 | Test Acc : 1.00000 | Zero : 2667 | One : 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.56it/s] [13:55<2:44:26, 21.40s/it]\n",
      "  8%|▊         | 40/500 [14:16<2:45:01, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Train Loss : 0.00040 | Test Acc : 1.00000 | Zero : 2667 | One : 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 78.76it/s] [17:28<2:40:04, 21.30s/it]\n",
      " 10%|█         | 50/500 [17:50<2:41:14, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Train Loss : 0.00034 | Test Acc : 1.00000 | Zero : 2667 | One : 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.45it/s] [21:03<2:37:46, 21.47s/it]\n",
      " 12%|█▏        | 60/500 [21:25<2:38:07, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Train Loss : 0.00049 | Test Acc : 1.00000 | Zero : 2667 | One : 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.92it/s] [24:36<2:32:40, 21.25s/it]\n",
      " 14%|█▍        | 70/500 [24:58<2:33:22, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Train Loss : 0.00034 | Test Acc : 1.00000 | Zero : 2667 | One : 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 69.53it/s] [28:09<2:29:05, 21.25s/it]\n",
      " 16%|█▌        | 80/500 [28:31<2:30:48, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Train Loss : 0.00021 | Test Acc : 1.00000 | Zero : 2667 | One : 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.44it/s] [31:42<2:25:19, 21.21s/it]\n",
      " 18%|█▊        | 90/500 [32:04<2:25:59, 21.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Train Loss : 0.00257 | Test Acc : 0.99963 | Zero : 2665 | One : 2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 69.58it/s] [35:17<2:22:34, 21.33s/it]\n",
      " 20%|██        | 100/500 [35:39<2:23:34, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Train Loss : 0.00196 | Test Acc : 0.99982 | Zero : 2666 | One : 2741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 80.64it/s]0 [38:52<2:19:45, 21.45s/it]\n",
      " 22%|██▏       | 110/500 [39:14<2:20:27, 21.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110 | Train Loss : 0.00151 | Test Acc : 0.99982 | Zero : 2666 | One : 2741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 79.92it/s]0 [42:27<2:16:32, 21.50s/it]\n",
      " 24%|██▍       | 120/500 [42:49<2:16:59, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120 | Train Loss : 0.00251 | Test Acc : 0.99982 | Zero : 2666 | One : 2741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.60it/s]0 [46:01<2:11:29, 21.26s/it]\n",
      " 26%|██▌       | 130/500 [46:23<2:11:58, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Train Loss : 0.00228 | Test Acc : 0.99963 | Zero : 2665 | One : 2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 82.57it/s]0 [49:35<2:08:35, 21.37s/it]\n",
      " 28%|██▊       | 140/500 [49:57<2:08:58, 21.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140 | Train Loss : 0.00273 | Test Acc : 0.99963 | Zero : 2665 | One : 2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 148/500 [52:47<2:05:04, 21.32s/it]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "epochs = 500\n",
    "step = 100\n",
    "best_acc = 0.75\n",
    "model.load_state_dict(torch.load('./pseudo/result_100.pt'))\n",
    "for epoch in trange(epochs):\n",
    "    for i, unlabeled in enumerate(p_dataloader):\n",
    "        input = unlabeled['input_ids'].to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        output_unlabeled = model(input)\n",
    "        _, pseudo_labeled = torch.max(output_unlabeled, 1)\n",
    "        \n",
    "        model.train()\n",
    "        output = model(input)\n",
    "        unlabeled_loss = alpha_weight(step) * F.cross_entropy(output, pseudo_labeled)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        unlabeled_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            for j, batch in enumerate(dataloader):\n",
    "                input = batch['input_ids'].to(device)\n",
    "                label = batch['label'].to(device)\n",
    "                output = model(input)\n",
    "                loss = F.cross_entropy(output, label)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            step += 1\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        correct, loss = 0, 0\n",
    "        zero, one = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_dataloader):\n",
    "                data = batch['input_ids'].cuda()\n",
    "                labels = batch['label']\n",
    "                output = model(data)\n",
    "                predicted = torch.max(output,1)[1]\n",
    "                \n",
    "                zero += predicted.tolist().count(0)\n",
    "                one += predicted.tolist().count(1)\n",
    "                \n",
    "                correct += (predicted==labels.cuda()).sum()\n",
    "                loss += F.cross_entropy(output, labels.cuda()).item()\n",
    "        \n",
    "        print(f'Epoch: {epoch+1} | Alpha : {alpha_weight(step)} | Train Loss : {loss/len(test_dataloader):.5f} | Test Acc : {correct/len(test_dataset):.5f} | Zero : {zero} | One : {one}')\n",
    "        \n",
    "        eval_acc = correct/len(test_dataloader)\n",
    "        if eval_acc > best_acc:\n",
    "            torch.save(model.state_dict(), f'./save/pseudo/result.pt')\n",
    "            best_acc = eval_acc\n",
    "        \n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './pseudo/result.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(500000, 100)\n",
       "  (conv1): Conv1d(100, 128, kernel_size=(2,), stride=(1,))\n",
       "  (conv2): Conv1d(100, 128, kernel_size=(4,), stride=(1,))\n",
       "  (conv3): Conv1d(100, 128, kernel_size=(6,), stride=(1,))\n",
       "  (fc): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (lstm): LSTM(100, 128, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (dropout2): Dropout(p=0.4, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (AvgPool): AdaptiveAvgPool1d(output_size=128)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./pseudo/result_100.pt'))\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 15587, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text = '700'\n",
    "\n",
    "tokens = tokenizer.encode(text).ids\n",
    "\n",
    "if len(tokens) <= 200:\n",
    "    for i in range(200-len(tokens)): tokens.append(0)\n",
    "elif len(tokens) > 200:\n",
    "    for i in range(len(tokens)-200): tokens.pop()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8995, 0.1005]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lightweight/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([tokens]).cuda()\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "print(F.softmax(output))\n",
    "print(torch.argmax(output, -1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e31c68abf1d5dd3f9e2269f23eadf1b199587e56c0618a30760176a65ebfcab4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('lightweight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
