{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment preprocessing\n",
    "- 숫자는 욕설로 만들 수 있음\n",
    "- 초성만으로 이루어진 단어또한 비웃음이나 욕설로 만들 수 있음\n",
    "- 이모티콘은 실제 사용에서는 어떨지 모르나 데이터로 들어올 때는 학습에 방해될 것으로 예상됨(UNK 토큰으로 판단될 수 있다고 생각함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Reference : https://gist.github.com/kse0202/9d3d8d519170064cefdd12fcb718afa0\n",
    "\"\"\"\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    \n",
    "    for i in trange(len(dataset)):\n",
    "        comment = dataset.iloc[i, 0]\n",
    "        # checking nan\n",
    "        if comment != comment:\n",
    "            continue\n",
    "        \n",
    "        tmp = comment\n",
    "        comment = re.sub(r\"[“”‘’\\\"\\']\", r\"\\'\", comment)\n",
    "        comment = re.sub(r\"[〈<＜「≪《『]\", \"<\", comment)\n",
    "        comment = re.sub(r\"[〉>＞」≫》』]\", \">\", comment)\n",
    "        comment = re.sub(r'[‥…]+', r'...', comment)\n",
    "        comment = re.sub(r'[\\?¿？]+', r'\\?', comment)\n",
    "        comment = re.sub(r'[!¡！]+', r'!', comment)\n",
    "        comment = re.sub(r\"([^\\-—_=+,\\./<>?\\[\\]{};:\\'\\\"!@#$%\\^&*\\(\\)₩`´~\\|\\\\ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔゞァ-・ヽヾ゛゜ー一-龯\\u3000-\\u303F\\u3400-\\u4DBF\\u4E00-\\u9FFF\\s]+)\", r'', comment)\n",
    "        comment = re.sub(r\"\\s+\", r\" \", comment)\n",
    "        if len(comment) <= 1 and tmp != comment:\n",
    "            comment = np.nan\n",
    "        elif len(comment) > 500:\n",
    "            comment = np.nan\n",
    "        \n",
    "        dataset.iloc[i, 0] = comment\n",
    "        \n",
    "    print(\"checking nan\")\n",
    "    print(sum(dataset['text'].isna()), \"number of nan exist\")\n",
    "    dataset = dataset[dataset['text'].notna()]\n",
    "    print(\"checking null\")\n",
    "    print(sum(dataset['text'].isnull()), \"number of null exist\")\n",
    "    dataset = dataset[dataset['text'].notnull()]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "quotes = re.compile(r\"[“”‘’\\\"\\']\")\n",
    "l_bracket = re.compile(r\"[〈<＜「≪《『]\")\n",
    "r_bracket = re.compile(r\"[〉>＞」≫》』]\")\n",
    "dots = re.compile(r'[‥…]+')\n",
    "question = re.compile(r'[\\?¿？]+')\n",
    "exclamation = re.compile(r'[!¡！]+')\n",
    "remainders = re.compile(r\"([^\\-—_=+,\\./<>?\\[\\]{};:\\'\\\"!@#$%\\^&*\\(\\)₩`´~\\|\\\\ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔゞァ-・ヽヾ゛゜ー一-龯\\u3000-\\u303F\\u3400-\\u4DBF\\u4E00-\\u9FFF\\s]+)\")\n",
    "multiple_spaces = re.compile(r\"\\s+\")\n",
    "\n",
    "def preprocessing(dataset):\n",
    "    \n",
    "    for i in trange(len(dataset)):\n",
    "        comment = dataset.iloc[i, 0]\n",
    "        # checking nan\n",
    "        if comment != comment:\n",
    "            continue\n",
    "        \n",
    "        tmp = comment\n",
    "        comment = quotes.sub(r\"\\'\", comment)\n",
    "        comment = l_bracket.sub(\"<\", comment)\n",
    "        comment = r_bracket.sub(\">\", comment)\n",
    "        comment = dots.sub(r'...', comment)\n",
    "        comment = question.sub(r'\\?', comment)\n",
    "        comment = exclamation.sub(r'!', comment)\n",
    "        comment = remainders.sub(r'', comment)\n",
    "        comment = multiple_spaces.sub(r\" \", comment)\n",
    "        if len(comment) <= 1 and tmp != comment:\n",
    "            comment = np.nan\n",
    "        elif len(comment) > 500:\n",
    "            comment = np.nan\n",
    "        \n",
    "        dataset.iloc[i, 0] = comment\n",
    "        \n",
    "    print(\"checking nan\")\n",
    "    print(sum(dataset['text'].isna()), \"number of nan exist\")\n",
    "    dataset = dataset[dataset['text'].notna()]\n",
    "    print(\"checking null\")\n",
    "    print(sum(dataset['text'].isnull()), \"number of null exist\")\n",
    "    dataset = dataset[dataset['text'].notnull()]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건 ㅇㅂ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다 10년 더 입어야지 ㅋㅋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고 ㅋㅋㅋㅋ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text\n",
       "0                                    좌배 까는건 ㅇㅂ\n",
       "1                  집에 롱 패딩만 세 개다 10년 더 입어야지 ㅋㅋ\n",
       "2  개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아\n",
       "3                                  세탁이라고 봐도 된다\n",
       "4                            애새끼가 초딩도 아니고 ㅋㅋㅋㅋ"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "curse_dataset = pd.read_csv('curse.tsv', sep='\\t')\n",
    "beep_dataset = pd.read_csv('beepData.tsv', sep='\\t')\n",
    "twitch_dataset = pd.read_csv('chatData.tsv', sep='\\t')\n",
    "train_dataset = pd.concat([curse_dataset[['text']], beep_dataset, twitch_dataset[['text']]], ignore_index=True)\n",
    "train_dataset = train_dataset[train_dataset['text'].notna()]\n",
    "train_dataset = train_dataset[train_dataset['text'].notnull()]\n",
    "print(len(train_dataset))\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1457392/1457392 [03:53<00:00, 6251.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking nan\n",
      "3 number of nan exist\n",
      "checking null\n",
      "0 number of null exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1457389"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = preprocessing(train_dataset)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def punctuation(dataset):\n",
    "    \n",
    "    \"\"\"punctuation preprocessing.\"\"\"\n",
    "    \"\"\"텍스트 길이의 10%~20%를 punctuation 삽입하여 모델이 robust하도록 한다.\"\"\"\n",
    "    new_dataset = []\n",
    "    punc = ['.',',',\"'\",';','/','-','~','!','@','?','^',' ']\n",
    "    \n",
    "    for i in trange(len(dataset)):\n",
    "        text = dataset[i]\n",
    "        if len(text) <= 30 and random.random() < 0.1: # 0.3 확률로 띄어쓰기 없애기 (길이 30 이하 텍스트만)\n",
    "            text = ''.join(text.split())\n",
    "            \n",
    "        if random.random() < 0.3: # 0.4 확률로 punctuation 추가\n",
    "            punc_size = random.randint(max(len(text)//20, 3), max(len(text)//10, 3)) # 모든 텍스트에 최소 3개는 들어가도록\n",
    "            text = list(text)\n",
    "            for _ in range(punc_size):\n",
    "                txt_rnd = random.randint(0, len(text)-1)\n",
    "                punc_rnd = random.randint(0, len(punc)-1)\n",
    "                text.insert(txt_rnd, punc[punc_rnd])\n",
    "            new_dataset.append(''.join(text).replace(\"  \", \" \"))\n",
    "        else:\n",
    "            new_dataset.append(text)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1457389/1457389 [00:05<00:00, 253251.18it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 254305.81it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255173.16it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255572.86it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256114.42it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255924.72it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255557.70it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257366.78it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256134.05it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 254934.56it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 254657.25it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 254727.83it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258357.60it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258186.29it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257977.89it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 254807.47it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255640.91it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257544.43it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257256.71it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256355.53it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257471.30it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258220.07it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256880.32it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258920.00it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257464.47it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258119.00it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256661.81it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255090.07it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256255.09it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258517.13it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257224.05it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258617.74it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255528.53it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258199.79it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256207.64it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 254809.52it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257860.23it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258135.03it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259544.46it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259072.97it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258024.08it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259262.67it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259576.21it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257708.47it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256017.12it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257959.72it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259595.13it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258471.75it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258139.25it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257020.83it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257502.19it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258049.29it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257233.45it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259560.27it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257757.54it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257416.57it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 260213.93it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 260671.89it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259576.41it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257625.71it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258551.25it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259285.65it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258405.46it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259617.03it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257364.22it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257419.02it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257069.34it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257734.75it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256000.42it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 255467.59it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 256220.12it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258244.76it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259305.94it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258326.06it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258521.49it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 260155.07it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259221.63it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258353.44it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258429.52it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258296.51it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258246.16it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257128.13it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258567.85it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259559.55it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257545.33it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257159.74it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258116.50it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257572.19it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258626.93it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259232.50it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257937.70it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257192.88it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259027.70it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 259952.14it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258070.72it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 260213.36it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257561.32it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 260166.18it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 258348.94it/s]\n",
      "100%|██████████| 1457389/1457389 [00:05<00:00, 257118.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "145738900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_df['text'].to_list()\n",
    "text_list = []\n",
    "for _ in range(100):\n",
    "    text_list += punctuation(text)\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['좌-배 ~까는건 ㅇㅂ',\n",
       " \"집에,' 롱 패딩만 세 개다 10년 더' 입어야지 ㅋㅋ\",\n",
       " '개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아',\n",
       " '세탁이라고 봐도 된다',\n",
       " '애새끼가 초딩도 아니고 ㅋㅋㅋㅋ',\n",
       " '731부대의 후예라 그런지 가학적인 아이디어는 세계최고임 이래서 애교만 떨어도 돈 벌리는 한국에 기를 써서 진출하려고 하지조센남자들은 또 이쁜여자만 보면 사족을 못쓰며 공주대접해주는 놈들이니',\n",
       " '재앙이한건햇노',\n",
       " '글쓴이 와꾸 승리에 비하면 방사능 피폭 원숭이 일듯',\n",
       " '마 씨발련 아 몇평이고 맷개드갔노 니 대하이햄하고 해밨나',\n",
       " '은행에 대출 상담 받으러 가보면 직업의 귀천 바로 알려줌']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'none': 0, 'label': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_num = dict(\n",
    "    none = 0, label = 1\n",
    ")\n",
    "label_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'none', 1: 'label'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_label = {}\n",
    "for k in label_to_num.keys():\n",
    "    num_to_label[label_to_num[k]]=k\n",
    "num_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('./label_to_num', 'rb') as f:\n",
    "#     label_to_num = pickle.load(f)\n",
    "\n",
    "# with open('./num_to_label', 'rb') as f:\n",
    "#     num_to_label = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tocsv(dataset):\n",
    "#     df = pd.DataFrame(columns=['text','label'])\n",
    "#     for i in trange(len(dataset)):\n",
    "#         df=df.append(\n",
    "#             dict(\n",
    "#                 text=dataset['text'][i],\n",
    "#                 label=(0 if dataset['curse'][i]==0 else 1)\n",
    "#             )\n",
    "#             ,ignore_index=True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = tocsv(train_dataset)\n",
    "# eval_df = tocsv(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train dataset none 개수 : {}, curse 개수 : {}'.format(list(train_df['label']).count(0), list(train_df['label']).count(1)))\n",
    "# print('eval dataset none 개수 : {}, hate 개수 : {}'.format(list(eval_df['label']).count(0), list(eval_df['label']).count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(index=train_df[train_df['text'] == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./train.tsv', sep='\\t')\n",
    "# eval_df.to_csv('./eval_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이저를 학습하지 말고 사전학습된 토크나이저를 사용하자.  \n",
    "  \n",
    "왜냐하면 문장내 어떤 단어가 올지 모르겠음 -> 단어가 더 많은 사전학습된 토크나이저가 유리하다고 판단  \n",
    "  \n",
    "또한, 전처리 부분에 있어서도 사전학습된 토크나이저가 성능이 좋은듯  \n",
    "  \n",
    "하지만 욕설에 대해서 토크나이징이 어떨지는 테스트해야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding = []\n",
    "# unk_ = []\n",
    "# for text in tqdm(list(train_df['text'])):\n",
    "#     tokens = tokenizer(text)['input_ids']\n",
    "#     if 1 in tokens:\n",
    "#         tokenized = tokenizer.decode(tokens)[6:-6]\n",
    "#         # print(text, tokenized)\n",
    "#         tokenized = re.sub(\"\\[UNK\\]\", r\"(.+)\", tokenized)\n",
    "#         try:\n",
    "#             unknown = re.match(tokenized, text)\n",
    "#         except:\n",
    "#             unknown = None\n",
    "#         if unknown:\n",
    "#             adding.append(unknown.group(1).strip())\n",
    "#         unk_.append([text, tokenizer.decode(tokens)])\n",
    "# unk_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19333506"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt_file = '\\n'.join(train_df.loc[:,\"text\"].values.tolist())\n",
    "# len(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = '\\n'.join(text_list)\n",
    "len(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sentence.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    f.write(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145738900"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./sentence.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    asdfg = f.read().splitlines()\n",
    "    \n",
    "len(asdfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "special = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "unused = [f'[unused{i}]' for i in range(300)]\n",
    "\n",
    "# load sentences\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")\n",
    "\n",
    "limit_alphabet = 10000\n",
    "vocab_size = 30000\n",
    "\n",
    "tokenizer.train(\n",
    "    files='./sentence.txt', # 문장들을 모아놓음\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "    special_tokens = special + unused,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=limit_alphabet,\n",
    ")\n",
    "\n",
    "tokenizer.save(\"./tokenizer.json\", pretty=True)  # save tokenizer.json, pretty=True로 두시면 json 형식이 보기좋게 저장됩니다\n",
    "tokenizer.save_model('./')  # save vocab.txt\n",
    "\n",
    "# from pretrained(\"./vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        BertWordPieceTokenizer\n",
       "\u001b[0;31mString form:\u001b[0m Tokenizer(vocabulary_size=30000, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token <...> text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False, wordpieces_prefix=##)\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/conda/envs/lightweight/lib/python3.7/site-packages/tokenizers/implementations/bert_wordpiece.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   Bert WordPiece Tokenizer \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./vocab.txt\", \"r\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding_prep = []\n",
    "\n",
    "# for token in tqdm(adding):\n",
    "    \n",
    "#     token = re.sub(r\"[ㄱ-ㅎㅏ-ㅣ0-9]+\", r\"\", token)\n",
    "#     token = re.sub(r\"[ㄱ-ㅎㅏ-ㅣ]+\", r\"\", token)\n",
    "#     token = re.sub(r\"[ㄱ-ㅎㅏ-ㅣ]+\", r\"\", token)\n",
    "    \n",
    "#     if re.fullmatch(r\"[ぁ-ゔゞァ-・ヽヾ゛゜ー一-龯]+\", token): # w, 숫자, 일본어만 있는 문장 제거\n",
    "#         continue\n",
    "        \n",
    "#     elif re.fullmatch(r\"[\\u3000-\\u303F\\u3400-\\u4DBF\\u4E00-\\u9FFF\\d\\s]+\", token): # 영어, 숫자, 중국어만 있는 문장 제거\n",
    "#         continue\n",
    "    \n",
    "#     rep = check_repeat(token)\n",
    "#     if rep: token = rep\n",
    "    \n",
    "#     while re.search(r\"(.)\\1{4,}\", fr\"{user_chat}\"):  # 4번 이상 반복되는 글자 3번만 반복되도록 수정\n",
    "#         repeated = re.search(r\"(.)\\1{4,}\", fr\"{user_chat}\").group()\n",
    "#         if repeated[0] == '^':\n",
    "#             user_chat = re.sub(r'\\^+', r'^^^', user_chat)\n",
    "#         try:\n",
    "#             user_chat = re.sub(repeated, repeated[0]*3, user_chat)\n",
    "#         except: break\n",
    "            \n",
    "#     user_chat = re.sub(r\"\\\\\", \"\", user_chat) # 백슬래쉬 제거\n",
    "    \n",
    "#     chat_log.append((user_id, user_chat))\n",
    "\n",
    "# len(chat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1365"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding += ['갠탆앗음', '얼마정도ᄇᆞ는데', 'ㄱㅐ씹죶간이고', '꺼늏다보니', '쓉쒱키는', '黄']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens(adding)\n",
    "# tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가해야 할 단어들\n",
    "- 갠탆앗음\n",
    "- 얼마정도ᄇᆞ는데\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "lightweight",
   "language": "python",
   "name": "lightweight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
